{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from random_word import Wordnik\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_strings(num_items: int, counter: int):\n",
    "    '''generate random words as search strings for lexica'''    \n",
    "    batch_size = 10\n",
    "    num_calls = math.ceil(num_items/batch_size)\n",
    "    output = []\n",
    "    wordnik_service = Wordnik()\n",
    "    for i in range(num_calls):\n",
    "        try:\n",
    "            # Return a single random word\n",
    "            res = wordnik_service.get_random_words(includePartOfSpeech =\"noun,verb,adverb\",hasDictionaryDef=True, limit=batch_size)\n",
    "            #TODO: Check if adding a duplicate search term\n",
    "            [output.append(x) for x in res]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print('Generated ', int(batch_size * num_calls), ' search terms.' )\n",
    "    return output\n",
    "\n",
    "def lexica_search(terms: list, counter: int):\n",
    "    '''search and store lexica results via their locked-down and rate-limited api'''\n",
    "    search_base='https://lexica.art/api/v1/search?q='\n",
    "    prompts = pd.DataFrame(columns=['search_string','source','prompt'])\n",
    "    print('Starting counter is: ', counter)\n",
    "    for i in range(counter,len(terms)):\n",
    "        #print('Searching term: ' , item)\n",
    "        query = terms[i]\n",
    "        query = query.replace(' ', '+')\n",
    "        try:\n",
    "            d = requests.get(url=(search_base + query))\n",
    "            data = d.json()\n",
    "            obj = data['images']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Writing counter to file: ', counter)\n",
    "            time.sleep(35)\n",
    "            f = open('./counter.txt', 'w')\n",
    "            f.write(str(counter))\n",
    "            f.close()\n",
    "            return prompts, counter\n",
    "        #print('Adding items to db for search term: ', item)\n",
    "        for item in obj:\n",
    "            row = [query, item['src'], item['prompt']]\n",
    "            prompts.loc[item['id']] = row\n",
    "        counter +=1\n",
    "        print('Commited prompts for term ', counter, ' out of ', len(terms))\n",
    "        time.sleep(.5)\n",
    "    f = open('./counter.txt', 'w')\n",
    "    f.write(counter)\n",
    "    f.close()\n",
    "    return prompts, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a database of prompts for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./1000-most-common.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    common = [line.rstrip() for line in lines]\n",
    "with open('./counter.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    counter = int(lines[0])\n",
    "\n",
    "while counter != (len(common)-1):\n",
    "    print(\"Starting the procedure again with counter: \", counter)\n",
    "    res_common, counter = lexica_search(terms = common, counter = counter)\n",
    "    filename = './prompts-with-common-' + str(counter) + '.json'\n",
    "    res_common.to_json(filename, orient='split')\n",
    "\n",
    "\n",
    "#res.to_json('./common-df.json',orient='split')\n",
    "res = pd.read_json('./common-df.json', orient='split')\n",
    "master = pd.read_json('./master-prompts.json', orient='split')\n",
    "\n",
    "full = pd.concat([master,res])\n",
    "full.shape\n",
    "full.to_csv('./full-prompts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing prompts w/spacy\n",
    "full = pd.read_json('./full-prompts.json', orient='split')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenize(prompt):\n",
    "    temp = []\n",
    "    f = nlp(prompt)\n",
    "    for ent in f.ents:\n",
    "        temp.append({'token': ent.text,'char_start': ent.start_char, 'char_end': ent.end_char, 'label': None, 'is_weak_label': False, 'pos': ent.label_})\n",
    "    return temp\n",
    "\n",
    "full['tokens'] = full['prompt'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found annotation match\n",
      "df row:  search_string                                            provokers\n",
      "source           https://lexica-serve-encoded-images.sharif.wor...\n",
      "prompt           a special operations member that looks like co...\n",
      "tokens           [{'token': 'colin farrell', 'char_start': 44, ...\n",
      "Name: 0158539f-d69b-4328-b26c-0d3e22795c1c, dtype: object\n",
      "annotations row:  {'annotations': [{'value': {'start': 168, 'end': 192, 'text': 'cornelis van poelenburgh', 'labels': ['Artist']}, 'id': '-PCrj-bI4Z', 'from_name': 'label', 'to_name': 'text', 'type': 'labels', 'origin': 'manual'}, {'value': {'start': 197, 'end': 208, 'text': 'dosso dossi', 'labels': ['Artist']}, 'id': 'y01Q-cujWT', 'from_name': 'label', 'to_name': 'text', 'type': 'labels', 'origin': 'manual'}], 'data': {'Unnamed: 0': '0158539f-d69b-4328-b26c-0d3e22795c1c', 'search_string': 'provokers', 'source': 'https://lexica-serve-encoded-images.sharif.workers.dev/md/0158539f-d69b-4328-b26c-0d3e22795c1c', 'prompt': 'a special operations member that looks like colin farrell and brad pitt, in battle, covert military pants, military boots, greek mythology, oil reinassance painting by cornelis van poelenburgh and dosso dossi, ultra detailed, concept art, 8 k what '}}\n"
     ]
    }
   ],
   "source": [
    "#Adding GT for the prompts based off of labels from Label Studio\n",
    "import json\n",
    "f = open('./gt.json')\n",
    "gt_file = json.load(f)\n",
    "#print('Example entry: ', gt_file[58])\n",
    "\n",
    "filtered = []\n",
    "for x in gt_file:\n",
    "    filtered.append({'annotations': x['annotations'][0]['result'],'data': x['data']})\n",
    "#print(filtered[58])\n",
    "\n",
    "full['gt_raw'] = None\n",
    "for i in range(full.shape[0]):\n",
    "    ss = full.index[i]\n",
    "    for item in filtered:\n",
    "        if item['data']['Unnamed: 0'] == ss:\n",
    "            #print('found annoation match')\n",
    "            #print('df row: ', full.loc[ss])\n",
    "            #print('annotations row: ', item)\n",
    "            full['gt_raw'][i] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-format annotations to work with Spacy\n",
    "for i in range(full.shape[0]):\n",
    "    for item in full.iloc[i]['tokens']:\n",
    "        try:\n",
    "            annotations = full.iloc[i]['gt_raw']['annotations']\n",
    "            #print(annotations)\n",
    "            for gt in annotations:\n",
    "                l = gt['value']\n",
    "                #print(l['start'])\n",
    "                #print(l['end'])\n",
    "                #print(abs(int(item['char_start']) - int(l['start'])) <= 3)\n",
    "                #print(item['char_end'])\n",
    "                if (abs(int(item['char_start']) - int(l['start'])) <= 3) or (abs(int(item['char_end']) - int(l['end'])) <= 3) :\n",
    "                    #print('Found an artist: ')\n",
    "                    #print(l['text'])\n",
    "                    item['label'] = 'artist'\n",
    "                else:\n",
    "                    item['label'] = 'other'\n",
    "        except TypeError as e:\n",
    "            pass  \n",
    "            #print('No annotations')\n",
    "full_labeled = full[full['gt_raw'].notna()]\n",
    "full = full.drop(['gt_raw'], axis=1)\n",
    "full_labeled = full_labeled.drop(['gt_raw'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Labeling with BART Large MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experiment: Can BART serve as a source of weak labeling for the end model?'''\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'james jean', 'char_start': 162, 'char_end': 172, 'label': 'artist', 'is_weak_label': True, 'pos': 'PERSON'}]\n"
     ]
    }
   ],
   "source": [
    "temp = full.iloc[222]['tokens']\n",
    "print(temp)\n",
    "labels = ['artist', 'other']\n",
    "threshold = 0.80\n",
    "for item in temp:\n",
    "    if item['label'] is None and item['pos'] == 'PERSON':\n",
    "        res = classifier(item['token'], labels)\n",
    "        print(res['sequence'],' : ', res['scores'][0])\n",
    "        if (res['scores'][0] > threshold):\n",
    "            item['label'] = 'artist'\n",
    "            item['is_weak_label'] = True\n",
    "            #print(item)\n",
    "            annotation = df['gt_raw'][222]\n",
    "            print(annotation)\n",
    "            annotation = annotation['annotations'].append(item)\n",
    "\n",
    "#Omitting adding these weak labels to the training set for now, since the model was able to get strong scores to start.\n",
    "'''for i in range(full.shape[0]):\n",
    "    temp = full.iloc[i]['tokens']\n",
    "    for item in temp:\n",
    "        if item['label'] is None and item['pos'] == 'PERSON':\n",
    "            res = classifier(item['token'], labels)\n",
    "            print(res['sequence'],' : ', res['scores'][0])\n",
    "            if (res['scores'][0] > threshold):\n",
    "                item['label'] = 'artist'\n",
    "                item['is_weak_label'] = True'''\n",
    "\n",
    "#full.to_json('./full-checkpoint.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data science ', []), ('extreme long shot view of a face in agony coming out of the ground by kentaro miura, hyper-detailed', [(70, 83, 'artist')]), ('lies we tell ourselves', []), ('Taco Bell if society was perfect.', [(0, 9, 'other')]), ('why do we die?', [])]\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe to Docs for training\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "#take full_labeled and parse out all the tokens and assign labels\n",
    "full_labeled['token_convert'] = None\n",
    "for i in range(full_labeled.shape[0]):\n",
    "    tmp = []\n",
    "    for item in full_labeled['tokens'][i]:\n",
    "        #print(item)\n",
    "        if item['label'] is None:\n",
    "            item['label'] = 'other'\n",
    "        tmp.append((item['char_start'], item['char_end'], item['label']))\n",
    "    full_labeled['token_convert'][i] = tmp\n",
    "#print(full_labeled['token_convert'][0])\n",
    "\n",
    "full_labeled.head()\n",
    "training_data = []\n",
    "for i in range(full_labeled.shape[0]):\n",
    "    training_data.append((full_labeled['prompt'][i],full_labeled['token_convert'][i]))\n",
    "#print(training_data[-10:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "#print(len(training_data))\n",
    "# the DocBin will store the example documents\n",
    "db = DocBin()\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp(text)\n",
    "    #print(doc)\n",
    "    ents = []\n",
    "    #print(annotations)\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        #print(span.text)\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "    #print(ents)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "db.to_disk(\"./train.spacy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78432f6f3f29575f54084a0f731ad3137b13602a47f5f61425d7f4341eae791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
