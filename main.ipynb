{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martymoesta/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from random_word import Wordnik\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to scrape prompts from Lexica API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_strings(num_items: int, counter: int):\n",
    "    '''generate random words as search strings for lexica'''    \n",
    "    batch_size = 10\n",
    "    num_calls = math.ceil(num_items/batch_size)\n",
    "    output = []\n",
    "    wordnik_service = Wordnik()\n",
    "    for i in range(num_calls):\n",
    "        try:\n",
    "            # Return a single random word\n",
    "            res = wordnik_service.get_random_words(includePartOfSpeech =\"noun,verb,adverb\",hasDictionaryDef=True, limit=batch_size)\n",
    "            #TODO: Check if adding a duplicate search term\n",
    "            [output.append(x) for x in res]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print('Generated ', int(batch_size * num_calls), ' search terms.' )\n",
    "    return output\n",
    "\n",
    "def lexica_search(terms: list, counter: int):\n",
    "    '''search and store lexica results via their locked-down and rate-limited api'''\n",
    "    search_base='https://lexica.art/api/v1/search?q='\n",
    "    prompts = pd.DataFrame(columns=['search_string','source','prompt'])\n",
    "    print('Starting counter is: ', counter)\n",
    "    for i in range(counter,len(terms)):\n",
    "        #print('Searching term: ' , item)\n",
    "        query = terms[i]\n",
    "        query = query.replace(' ', '+')\n",
    "        try:\n",
    "            d = requests.get(url=(search_base + query))\n",
    "            data = d.json()\n",
    "            obj = data['images']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Writing counter to file: ', counter)\n",
    "            time.sleep(35)\n",
    "            f = open('./counter.txt', 'w')\n",
    "            f.write(str(counter))\n",
    "            f.close()\n",
    "            return prompts, counter\n",
    "        #print('Adding items to db for search term: ', item)\n",
    "        for item in obj:\n",
    "            row = [query, item['src'], item['prompt']]\n",
    "            prompts.loc[item['id']] = row\n",
    "        counter +=1\n",
    "        print('Commited prompts for term ', counter, ' out of ', len(terms))\n",
    "        time.sleep(.5)\n",
    "    f = open('./counter.txt', 'w')\n",
    "    f.write(counter)\n",
    "    f.close()\n",
    "    return prompts, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a database of prompts for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the procedure again with counter:  927\n",
      "Starting counter is:  927\n",
      "Commited prompts for term  928  out of  1000\n",
      "Commited prompts for term  929  out of  1000\n",
      "Commited prompts for term  930  out of  1000\n",
      "Commited prompts for term  931  out of  1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m counter \u001b[39m!=\u001b[39m (\u001b[39mlen\u001b[39m(common)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting the procedure again with counter: \u001b[39m\u001b[39m\"\u001b[39m, counter)\n\u001b[0;32m---> 10\u001b[0m     res_common, counter \u001b[39m=\u001b[39m lexica_search(terms \u001b[39m=\u001b[39;49m common, counter \u001b[39m=\u001b[39;49m counter)\n\u001b[1;32m     11\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./prompts-with-common-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(counter) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     12\u001b[0m     res_common\u001b[39m.\u001b[39mto_json(filename, orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [10], line 45\u001b[0m, in \u001b[0;36mlexica_search\u001b[0;34m(terms, counter)\u001b[0m\n\u001b[1;32m     43\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCommited prompts for term \u001b[39m\u001b[39m'\u001b[39m, counter, \u001b[39m'\u001b[39m\u001b[39m out of \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(terms))\n\u001b[0;32m---> 45\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m.5\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./counter.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m f\u001b[39m.\u001b[39mwrite(counter)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('./1000-most-common.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    common = [line.rstrip() for line in lines]\n",
    "with open('./counter.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    counter = int(lines[0])\n",
    "\n",
    "while counter != (len(common)-1):\n",
    "    print(\"Starting the procedure again with counter: \", counter)\n",
    "    res_common, counter = lexica_search(terms = common, counter = counter)\n",
    "    filename = './prompts-with-common-' + str(counter) + '.json'\n",
    "    res_common.to_json(filename, orient='split')\n",
    "\n",
    "\n",
    "#res.to_json('./common-df.json',orient='split')\n",
    "res = pd.read_json('./common-df.json', orient='split')\n",
    "master = pd.read_json('./master-prompts.json', orient='split')\n",
    "\n",
    "full = pd.concat([master,res])\n",
    "full.shape\n",
    "full.to_csv('./full-prompts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing prompts w/spacy\n",
    "full = pd.read_json('./full-prompts.json', orient='split')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenize(prompt):\n",
    "    temp = []\n",
    "    f = nlp(prompt)\n",
    "    for ent in f.ents:\n",
    "        temp.append({'token': ent.text,'char_start': ent.start_char, 'char_end': ent.end_char, 'label': None, 'is_weak_label': False, 'pos': ent.label_})\n",
    "    return temp\n",
    "\n",
    "full['tokens'] = full['prompt'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Annotations\n",
    "\n",
    "For experimentation purposes, I used the community version of Label Studio to annotate ~230 prompts. The label schema is [ARTIST, OTHER]. Label studio ground truth needs to be transformed to a spacy-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding GT for the prompts based off of labels from Label Studio\n",
    "import json\n",
    "f = open('./gt.json')\n",
    "gt_file = json.load(f)\n",
    "#print('Example entry: ', gt_file[58])\n",
    "\n",
    "filtered = []\n",
    "for x in gt_file:\n",
    "    filtered.append({'annotations': x['annotations'][0]['result'],'data': x['data']})\n",
    "#print(filtered[58])\n",
    "\n",
    "full['gt_raw'] = None\n",
    "for i in range(full.shape[0]):\n",
    "    ss = full.index[i]\n",
    "    for item in filtered:\n",
    "        if item['data']['Unnamed: 0'] == ss:\n",
    "            #print('found annoation match')\n",
    "            #print('df row: ', full.loc[ss])\n",
    "            #print('annotations row: ', item)\n",
    "            full['gt_raw'][i] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#count = 0\\n#for i in range(df.shape[0]):\\n    #if df['gt_raw'][i] is not None:\\n        #for item in df['gt_raw'][i]['annotations']:\\n            #print(item['value'])\\n\\ntrim = df[df['gt_raw'].notnull()]\\ntrim.shape\\ntrim.to_json('./trim-df.json', orient='split')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#count = 0\n",
    "#for i in range(df.shape[0]):\n",
    "    #if df['gt_raw'][i] is not None:\n",
    "        #for item in df['gt_raw'][i]['annotations']:\n",
    "            #print(item['value'])\n",
    "\n",
    "trim = df[df['gt_raw'].notnull()]\n",
    "trim.shape\n",
    "trim.to_json('./trim-df.json', orient='split')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Labeling with BART Large MNLI\n",
    "\n",
    "Weak supervision is a helpful technique when working with few or no labeled examples. Here, I demonstrate using BART LLM as a source of weak signal for labeling. For each entity that has a \"PERSON\" part-of-speech tag from spacy, ask BART whether this person is an artist or not. If prob(Yes) > 0.85, weakly label example as ARTIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'james jean', 'char_start': 162, 'char_end': 172, 'label': 'artist', 'is_weak_label': True, 'pos': 'PERSON'}]\n"
     ]
    }
   ],
   "source": [
    "temp = full.iloc[222]['tokens']\n",
    "print(temp)\n",
    "labels = ['artist', 'other']\n",
    "threshold = 0.80\n",
    "for item in temp:\n",
    "    if item['label'] is None and item['pos'] == 'PERSON':\n",
    "        res = classifier(item['token'], labels)\n",
    "        print(res['sequence'],' : ', res['scores'][0])\n",
    "        if (res['scores'][0] > threshold):\n",
    "            item['label'] = 'artist'\n",
    "            item['is_weak_label'] = True\n",
    "            #print(item)\n",
    "            annotation = annotation['annotations'].append(item)\n",
    "\n",
    "#Omitting adding these weak labels to the training set for now, since the model was able to get strong scores to start.\n",
    "for i in range(full.shape[0]):\n",
    "    temp = full.iloc[i]['tokens']\n",
    "    for item in temp:\n",
    "        if item['label'] is None and item['pos'] == 'PERSON':\n",
    "            res = classifier(item['token'], labels)\n",
    "            print(res['sequence'],' : ', res['scores'][0])\n",
    "            if (res['scores'][0] > threshold):\n",
    "                item['label'] = 'artist'\n",
    "                item['is_weak_label'] = True\n",
    "\n",
    "#full.to_json('./full-checkpoint.json', orient = 'split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training\n",
    "Spacy needs data in it's Doc() object form. In this section, we trim the DF to only strongly labeled examples and convert the existing dataframe into docs and write it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'value': {'start': 168, 'end': 192, 'text': 'cornelis van poelenburgh', 'labels': ['Artist']}, 'id': '-PCrj-bI4Z', 'from_name': 'label', 'to_name': 'text', 'type': 'labels', 'origin': 'manual'}, {'value': {'start': 197, 'end': 208, 'text': 'dosso dossi', 'labels': ['Artist']}, 'id': 'y01Q-cujWT', 'from_name': 'label', 'to_name': 'text', 'type': 'labels', 'origin': 'manual'}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a d & d character portrait of a beautiful noble elf princess with blonde hair, regal jewellry and elegant dress by bowater, charlie ', [(115, 122, 'Artist'), (124, 131, 'Artist')]), ('nicki minaj hugged by barack obama from behind, soviet colored propaganda poster, highly detailed illustration ', [(0, 11, 'Other'), (48, 54, 'Other')]), ('a beautiful painting of an indigenous man blowing tobacco snuff into the nose of another man , fantasy art, matte painting, highly detailed', []), ('File sharing website design', []), ('rage comics meme from the year 2 0 3 0. ', [(22, 38, 'Other')]), ('Arsenal win the Premier League', []), ('traditional japanese tiger drawing by junji ito, ', [(38, 47, 'Artist'), (12, 20, 'Other')]), ('foundation with gold, silver, precious stones, wood, hay, straw', [])]\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "trim = pd.read_json('./trim-df.json', orient='split')\n",
    "#print(trim['prompt'][0])\n",
    "#print(trim['gt_raw'][0]['annotations'])\n",
    "\n",
    "''''\n",
    "[(\"a special operations member that looks like colin farrell and brad pitt, in battle, covert military pants, military boots, greek mythology, oil reinassance painting by cornelis van poelenburgh and dosso dossi, ultra detailed, concept art, 8 k what\",[(168,182,artist),(197,208,artist)]) ...]\n",
    "'''\n",
    "# Need to add 'other' annotations as assumed negative label\n",
    "\n",
    "#print(trim['tokens'][0])\n",
    "#print(trim['gt_raw'][0]['annotations'])\n",
    "\n",
    "for i in range(trim.shape[0]):\n",
    "    tokens = trim['tokens'][i]\n",
    "    existing_annotations = trim['gt_raw'][i]['annotations']\n",
    "    for item in tokens:\n",
    "        exists = False\n",
    "        for check in existing_annotations:\n",
    "            if item['char_start'] == check['value']['start'] or item['char_end'] == check['value']['end']:\n",
    "                exists = True\n",
    "        if exists == False:\n",
    "            # add token with 'other' label\n",
    "            existing_annotations.append({'value': {'start': item['char_start'], 'end': item['char_end'], 'labels':['Other'] }})\n",
    "\n",
    "training_data = []\n",
    "for i in range(trim.shape[0]):\n",
    "    anno = []\n",
    "    for item in trim['gt_raw'][i]['annotations']:\n",
    "        tmp = item['value']\n",
    "        add = (tmp['start'], tmp['end'], tmp['labels'][0])\n",
    "        anno.append(add)\n",
    "    training_data.append((trim['prompt'][i],anno))\n",
    "print(training_data[22:30])\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to Docs for training\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.blank(\"en\")\n",
    "db = DocBin()\n",
    "\n",
    "'''\n",
    "training_data = [\n",
    "  (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]),\n",
    "]\n",
    "'''\n",
    "\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp(text)\n",
    "    #print(doc)\n",
    "    ents = []\n",
    "    #print(text)\n",
    "    #print(annotations)\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        #print(span.text)\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    \n",
    "    db.add(doc)\n",
    "db.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an NER model\n",
    "Model training is managed via spacy config files (`prompt_config.cfg`) and the command line. See **training-pipeline-output.txt** for checkpoints, loss, and overall score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Evaluation with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "dataset = load_dataset(\"Gustavosta/Stable-Diffusion-Prompts\", split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "for item in dataset['Prompt'][0:100]:\n",
    "   eval_set.append(item)\n",
    "db = DocBin()\n",
    "for item in eval_set:\n",
    "    doc = nlp(item)\n",
    "    db.add(doc)\n",
    "db.to_disk('./eval-docs.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "After running `spacy-eval.sh`, the predictions are stored in the `eval` directory. Traditionally, I would label a GT eval set by hand. You'll see that model predictions for the holdout set are anecdotally highly accurate. Would generally look at some doc and token level metrics for an official score, and things like confusion matrix, class level errors, and precision/recall curve to determine how/where I would fine tune.\n",
    "\n",
    "Some novel applications of this model:\n",
    "* Prompt optimization: Artist names are analyzed against prompt outputs to determine optimal artist names and locations in a prompt\n",
    "* A model like this begins to attribute \"credit\" to the various artist's work who were included in the prompt. The concern of attribution for artists in prompts is an unresolved issue. A model like enables the conversation to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78432f6f3f29575f54084a0f731ad3137b13602a47f5f61425d7f4341eae791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
